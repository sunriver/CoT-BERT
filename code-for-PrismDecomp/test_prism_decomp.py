#!/usr/bin/env python3
"""
PrismDecompæ¨¡å‹æµ‹è¯•è„šæœ¬
æµ‹è¯•åŸºäºæ£±é•œåˆ†è§£çš„å¤šè¯­ä¹‰å¥å­è¡¨ç¤ºå­¦ä¹ æ¨¡å‹çš„åŸºæœ¬åŠŸèƒ½
"""

import sys
import os
import torch
import numpy as np
from transformers import AutoTokenizer, AutoConfig

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('..')

from prism_decomp_model import BertForPrismDecomp, SemanticDecomposer, MultiSemanticSPR, SPR_Module

def test_semantic_decomposer():
    """æµ‹è¯•è¯­ä¹‰åˆ†è§£å™¨"""
    print("æµ‹è¯•è¯­ä¹‰åˆ†è§£å™¨...")
    
    hidden_dim = 768
    num_semantics = 7
    batch_size = 4
    
    decomposer = SemanticDecomposer(hidden_dim, num_semantics)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    sentence_repr = torch.randn(batch_size, hidden_dim)
    
    # å‰å‘ä¼ æ’­
    semantic_reprs, orth_loss = decomposer(sentence_repr)
    
    print(f"è¾“å…¥å½¢çŠ¶: {sentence_repr.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {semantic_reprs.shape}")
    print(f"è¯­ä¹‰ç»´åº¦æ•°é‡: {semantic_reprs.shape[1]}")
    print(f"è½¯æ­£äº¤æŸå¤±: {orth_loss.item():.4f}")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    assert semantic_reprs.shape == (batch_size, num_semantics, hidden_dim), f"æœŸæœ›å½¢çŠ¶ {(batch_size, num_semantics, hidden_dim)}, å®é™…å½¢çŠ¶ {semantic_reprs.shape}"
    assert orth_loss.item() >= 0, "è½¯æ­£äº¤æŸå¤±åº”è¯¥éè´Ÿ"
    
    print("âœ… è¯­ä¹‰åˆ†è§£å™¨æµ‹è¯•é€šè¿‡!")
    return True

def test_spr_module():
    """æµ‹è¯•SPRæ¨¡å—"""
    print("æµ‹è¯•SPRæ¨¡å—...")
    
    hidden_dim = 768
    batch_size = 4
    
    spr_module = SPR_Module(hidden_dim)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    semantic_repr = torch.randn(batch_size, hidden_dim)
    
    # å‰å‘ä¼ æ’­
    processed_repr, spr_loss = spr_module(semantic_repr)
    
    print(f"è¾“å…¥å½¢çŠ¶: {semantic_repr.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {processed_repr.shape}")
    print(f"SPRæŸå¤±å€¼: {spr_loss.item():.4f}")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    assert processed_repr.shape == (batch_size, hidden_dim), f"æœŸæœ›å½¢çŠ¶ {(batch_size, hidden_dim)}, å®é™…å½¢çŠ¶ {processed_repr.shape}"
    assert spr_loss.item() > 0, "SPRæŸå¤±åº”è¯¥å¤§äº0"
    
    print("âœ… SPRæ¨¡å—æµ‹è¯•é€šè¿‡!")
    return True

def test_multisemantic_spr():
    """æµ‹è¯•å¤šè¯­ä¹‰SPRæ¨¡å‹"""
    print("æµ‹è¯•å¤šè¯­ä¹‰SPRæ¨¡å‹...")
    
    hidden_dim = 768
    num_semantics = 7
    batch_size = 4
    
    multisemantic_spr = MultiSemanticSPR(hidden_dim, num_semantics, lambda1=1.0, lambda2=0.01)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    sentence_repr = torch.randn(batch_size, hidden_dim)
    
    # å‰å‘ä¼ æ’­
    final_repr, total_loss, semantic_spr_losses = multisemantic_spr(sentence_repr)
    
    print(f"è¾“å…¥å½¢çŠ¶: {sentence_repr.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {final_repr.shape}")
    print(f"æ€»æŸå¤±: {total_loss.item():.4f}")
    print(f"è¯­ä¹‰SPRæŸå¤±æ•°é‡: {len(semantic_spr_losses)}")
    print(f"Î»â‚ (è‡ªæ­£åˆ™åŒ–æƒé‡): {multisemantic_spr.lambda1}")
    print(f"Î»â‚‚ (è½¯æ­£äº¤æƒé‡): {multisemantic_spr.lambda2}")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    assert final_repr.shape == (batch_size, hidden_dim), f"æœŸæœ›å½¢çŠ¶ {(batch_size, hidden_dim)}, å®é™…å½¢çŠ¶ {final_repr.shape}"
    assert len(semantic_spr_losses) == num_semantics, f"æœŸæœ›è¯­ä¹‰æŸå¤±æ•°é‡ {num_semantics}, å®é™…æ•°é‡ {len(semantic_spr_losses)}"
    assert total_loss.item() > 0, "æ€»æŸå¤±åº”è¯¥å¤§äº0"
    
    print("âœ… å¤šè¯­ä¹‰SPRæ¨¡å‹æµ‹è¯•é€šè¿‡!")
    return True

def test_bert_for_prism_decomp():
    """æµ‹è¯•å®Œæ•´çš„BERT PrismDecompæ¨¡å‹"""
    print("æµ‹è¯•BERT PrismDecompæ¨¡å‹...")
    
    try:
        # ä½¿ç”¨ä¸€ä¸ªç®€å•çš„é…ç½®è¿›è¡Œæµ‹è¯•
        config = AutoConfig.from_pretrained("bert-base-uncased")
        tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        
        # åˆ›å»ºæ¨¡å‹å‚æ•°
        class ModelArgs:
            def __init__(self):
                self.num_semantics = 7
                self.orthogonal_constraint = True
                self.semantic_weights_learnable = True
        
        model_args = ModelArgs()
        
        # åˆ›å»ºæ¨¡å‹
        model = BertForPrismDecomp(config, model_args=model_args)
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥ï¼ˆåŒå¥ç”¨äºå¯¹æ¯”å­¦ä¹ ï¼‰
        batch_size = 2
        num_sent = 2  # æ¯ä¸ªæ ·æœ¬åŒ…å«ä¸¤ä¸ªå¥å­
        seq_length = 32
        
        input_ids = torch.randint(0, tokenizer.vocab_size, (batch_size, num_sent, seq_length))
        attention_mask = torch.ones(batch_size, num_sent, seq_length)
        
        print(f"è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
        print(f"æ³¨æ„åŠ›æ©ç å½¢çŠ¶: {attention_mask.shape}")
        
        # å‰å‘ä¼ æ’­
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )
        
        print(f"è¾“å‡ºlogitså½¢çŠ¶: {outputs.logits.shape}")
        print(f"æ€»æŸå¤±å€¼: {outputs.loss.item():.4f}")
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        assert outputs.logits.shape == (batch_size, config.hidden_size), f"æœŸæœ›å½¢çŠ¶ {(batch_size, config.hidden_size)}, å®é™…å½¢çŠ¶ {outputs.logits.shape}"
        assert outputs.loss.item() > 0, "æ€»æŸå¤±åº”è¯¥å¤§äº0"
        
        print("âœ… BERT PrismDecompæ¨¡å‹æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ BERT PrismDecompæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_sentence_embedding():
    """æµ‹è¯•å¥å­åµŒå…¥åŠŸèƒ½"""
    print("æµ‹è¯•å¥å­åµŒå…¥åŠŸèƒ½...")
    
    try:
        # ä½¿ç”¨ä¸€ä¸ªç®€å•çš„é…ç½®è¿›è¡Œæµ‹è¯•
        config = AutoConfig.from_pretrained("bert-base-uncased")
        tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        
        # åˆ›å»ºæ¨¡å‹å‚æ•°
        class ModelArgs:
            def __init__(self):
                self.num_semantics = 7
                self.orthogonal_constraint = True
                self.semantic_weights_learnable = True
        
        model_args = ModelArgs()
        
        # åˆ›å»ºæ¨¡å‹
        model = BertForPrismDecomp(config, model_args=model_args)
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        sentences = ["This is a test sentence.", "Another test sentence for evaluation."]
        
        # ç¼–ç å¥å­
        inputs = tokenizer(sentences, return_tensors="pt", padding=True, truncation=True)
        
        print(f"è¾“å…¥å¥å­æ•°é‡: {len(sentences)}")
        print(f"è¾“å…¥å½¢çŠ¶: {inputs['input_ids'].shape}")
        
        # è·å–å¥å­åµŒå…¥
        with torch.no_grad():
            outputs = model(
                input_ids=inputs['input_ids'],
                attention_mask=inputs['attention_mask'],
                sent_emb=True,
                return_dict=True
            )
        
        print(f"å¥å­åµŒå…¥å½¢çŠ¶: {outputs.pooler_output.shape}")
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        assert outputs.pooler_output.shape[0] == len(sentences), f"æœŸæœ›å¥å­æ•°é‡ {len(sentences)}, å®é™…æ•°é‡ {outputs.pooler_output.shape[0]}"
        assert outputs.pooler_output.shape[1] == config.hidden_size, f"æœŸæœ›éšè—ç»´åº¦ {config.hidden_size}, å®é™…ç»´åº¦ {outputs.pooler_output.shape[1]}"
        
        print("âœ… å¥å­åµŒå…¥åŠŸèƒ½æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ å¥å­åµŒå…¥åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 60)
    print("PrismDecompæ¨¡å‹åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    tests = [
        test_semantic_decomposer,
        test_spr_module,
        test_multisemantic_spr,
        test_bert_for_prism_decomp,
        test_sentence_embedding,
    ]
    
    passed = 0
    total = len(tests)
    
    for test_func in tests:
        try:
            if test_func():
                passed += 1
            print()
        except Exception as e:
            print(f"âŒ æµ‹è¯• {test_func.__name__} å¤±è´¥: {e}")
            print()
    
    print("=" * 60)
    print(f"æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    print("=" * 60)
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! PrismDecompæ¨¡å‹å®ç°æ­£ç¡®!")
        return True
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°!")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
