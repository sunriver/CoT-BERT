model_name_or_path: "/Users/lmf/Documents/local/code/pretrain_models/bert-base-uncased"
train_file: "../data/wiki1m_for_simcse.txt"
output_dir: "../result/PrismDecomp-BERT"
num_train_epochs: 1
per_device_train_batch_size: 32
gradient_accumulation_steps: 1 # 每gradient_accumulation_steps个step更新一次梯度，相当于batch_size=32*gradient_accumulation_steps，
learning_rate: 5e-6
max_seq_length: 32
evaluation_strategy: steps
metric_for_best_model: stsb_spearman
load_best_model_at_end: true
eval_steps: 2 # 每eval_steps个更新梯度进行一次评估，即gradient_accumulation_steps
#save_steps: 125
overwrite_output_dir: true
do_train: true
fp16: false
preprocessing_num_workers: 10
use_mps_device: false
dataloader_drop_last: true
seed: 0
overwrite_cache: false

logging_steps: 2

# PrismDecomp specific arguments
num_semantics: 7
orthogonal_constraint: true
semantic_weights_learnable: true

# Template configuration
mask_embedding_sentence: true
mask_embedding_sentence_template: "The sentence of \"[X]\" means [MASK]."
