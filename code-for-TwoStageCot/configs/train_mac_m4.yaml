model_name_or_path: "/Users/lmf/Documents/local/code/pretrain_models/bert-base-uncased"
train_file: "../data/wiki1m_for_simcse.txt"
output_dir: "../result/TwoStageCoT-BERT"
num_train_epochs: 1
per_device_train_batch_size: 32
gradient_accumulation_steps: 1
learning_rate: 1e-5
max_seq_length: 32
evaluation_strategy: steps
metric_for_best_model: stsb_spearman
load_best_model_at_end: true

eval_steps: 2 # 每eval_steps个更新梯度进行一次评估，即gradient_accumulation_steps

# 补充保存策略配置
save_strategy: steps       # 与评估策略保持一致（按步数保存）
save_steps: 2             # 每 30 步保存一次（与 eval_steps 相同，确保每次评估后都保存）
save_total_limit: 1        # 只保留最新的1个模型（即当前最佳模型）

overwrite_output_dir: true
do_train: true
fp16: false
preprocessing_num_workers: 10
use_mps_device: false
dataloader_drop_last: true
seed: 0
overwrite_cache: true

logging_steps: 2

# TwoStageCoT specific arguments
temperature: 0.05

# Template configuration
mask_embedding_sentence: true
stage1_template: "The sentence of \"[X]\" means [MASK]."
stage2_template: "so [IT_SPECIAL_TOKEN] can be summarized as [MASK]."

